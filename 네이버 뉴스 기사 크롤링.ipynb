{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e658201",
   "metadata": {},
   "source": [
    "## 1. 데이터 수집\n",
    "네이버 뉴스 기사 사회면 헤드라인 웹크롤링 <br/>\n",
    "출처: https://news.naver.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c654aa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 불러오기\n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "3e534fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num():\n",
    "    print(\"##########################################\")\n",
    "    num = int(input(\"원하는 숫자를 입력해주세요.\"))\n",
    "    print(\"##########################################\")\n",
    "    return num\n",
    "\n",
    "def news_crawling():\n",
    "    ######### 네이버 url 가져올때, hearder을 안주는 경우 오류 발생시 ############\n",
    "    headers = {\n",
    "    \"User-Agent\":\n",
    "    \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) \\\n",
    "    Chrome/44.0.2403.157 Safari/537.36\"\n",
    "    }\n",
    "    print(\"############# 네이버 뉴스 헤드라인 웹 크롤링 시작 ############\")\n",
    "    ### 네이버 뉴스 메인 페이지에서 상단바에서 뉴스 토픽을 설정해서 해당 토픽의 'a' 태그로 이동\n",
    "    r = requests.get('https://news.naver.com/', headers=headers)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    for title in range(len(soup.find(\"ul\",'Nlnb_menu_list').find_all('li')[1:-4])):\n",
    "        print(title+1 ,'', soup.find(\"ul\",'Nlnb_menu_list').find_all('li')[1:-4][title].text)\n",
    "    title_num = num() # title 선택\n",
    "    title = soup.find(\"ul\",'Nlnb_menu_list').find_all('li')[title_num].text # 나중에 데이터 저장하기 위해서 \n",
    "    url = soup.find(\"ul\",'Nlnb_menu_list').find_all('li')[title_num].find('a')['href']\n",
    "    \n",
    "    ### 이동된 세부 뉴스 토픽 페이지에서 왼쪽 사이드 바에서 세부 토픽 선택 후 해당 'a' 태그로 이동\n",
    "    r = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    for topic in range(len(soup.find('div',id='snb').find_all('li'))):\n",
    "        print(topic+1 ,'', soup.find('div',id='snb').find_all('li')[topic].text)    \n",
    "    topic_num = num()\n",
    "    \n",
    "    # a 태그로 이동하지 않고 / 한글을 입력하면 전체 뉴스 기사가 나옴\n",
    "    a = int(input(\"요약본 1번 / 전체본 2번 선택해주세요.\"))\n",
    "    if a == 1:\n",
    "        minor_topic = soup.find('div',id='snb').find_all('li')[topic_num-1].find('a')['href'].split('&')[-1]\n",
    "    else:\n",
    "        minor_topic = soup.find('div',id='snb').find_all('li')[topic_num-1].text\n",
    "    minor_topic1 = soup.find('div',id='snb').find_all('li')[topic_num-1].text # 나중에 데이터 저장할때 쓰기위함\n",
    "    \n",
    "    news_list = []\n",
    "    date_list = []\n",
    "    # 중간에 url이 main에서 list로 바껴야함\n",
    "    news_url = url+'&'+minor_topic\n",
    "    news_url = news_url.split('main.')[0]+'list.'+news_url.split('main.')[1]\n",
    "    news_url = news_url.replace('LSD','LS2D') \n",
    "    print(news_url)\n",
    "    \n",
    "    ###################### 날짜 입력하는 부분은 안배움 ############################\n",
    "    ###################### 밑 날짜 데이터 다루기 예시 참조 ########################\n",
    "    print(\"##########################################\")\n",
    "    print('뉴스 크롤링을 시작할 날짜를 입력해주세요.')\n",
    "    start = input(\"예: 20200101\")\n",
    "    print('뉴스 크롤링을 끝낼 날짜를 입력해주세요.')\n",
    "    end = input(\"예: 20200101\")\n",
    "    print(\"##########################################\")\n",
    "    start = datetime.datetime.strptime(start, \"%Y%m%d\")\n",
    "    end = datetime.datetime.strptime(end, \"%Y%m%d\")\n",
    "    period = end-start\n",
    "    date_generated = pd.date_range(start,periods=period.days)\n",
    "    ###############################################################################\n",
    "    \n",
    "    # 날짜 별로 이동하기 위한 for문\n",
    "    for date in range(len(date_generated)):\n",
    "        r = requests.get(news_url+'&date='+str(date_generated[date].strftime(\"%Y%m%d\")), headers=headers)\n",
    "        print(news_url+'&date='+str(date_generated[date].strftime(\"%Y%m%d\")))\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        # 끝 페이지를 알기위해서 임의의 높은 숫자를 입력하면 맨 끝 페이지로 이동 [실제 10000번째가 있으면 끝 아닐 수 있음]\n",
    "        s = requests.get(news_url+'&date='+str(date_generated[date].strftime(\"%Y%m%d\"))+\n",
    "                         '&page=100000', headers=headers)\n",
    "        s_soup = BeautifulSoup(s.text, 'html.parser')\n",
    "        page_num = int(s_soup.find('div','paging').find('strong').text)\n",
    "        ###### tqdm()을 for문에 사용하면 for문이 돌아가는 것을 시각화 할 수 있음.\n",
    "        ## 페이지를 넘기는 부분\n",
    "        for j in tqdm(range(page_num),desc=date_generated[date].strftime(\"%Y%m%d\")):\n",
    "            r = requests.get(news_url+'&date='+str(date_generated[date].strftime(\"%Y%m%d\")+\n",
    "                             '&page='+str(j+1)), headers=headers)\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "            # 각 페이지 마다 기사를 가져오기 위한 부분\n",
    "            for i in range(len(soup.find('div','list_body newsflash_body').find_all('li'))):\n",
    "                date_list.append(date_generated[date].strftime(\"%Y%m%d\"))\n",
    "                news_list.append(soup.find('div','list_body newsflash_body').find_all('li')[i].find_all('a')[-1].text.strip())\n",
    "    \n",
    "    # 데이터 프레임으로 저장\n",
    "    df = pd.DataFrame({\"날짜\":date_list,\"헤드라인\":news_list})\n",
    "    \n",
    "    # 데이터 프레임 이름 설정에 / 가 있으면 경로 오류가 생김으로 변경\n",
    "    if '/' in minor_topic1 or '/' in title:\n",
    "        title = title.replace('/','')\n",
    "        minor_topic1 = minor_topic1.replace('/','')\n",
    "        \n",
    "    # 데이터 저장하기 [경로 맞게 지정해주기]\n",
    "    df.to_csv('dataset/네이버뉴스_'+title+'_'+minor_topic1+'.csv',encoding='utf-8-sig')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "1a9d8d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# 네이버 뉴스 헤드라인 웹 크롤링 시작 ############\n",
      "1  정치\n",
      "2  경제\n",
      "3  사회\n",
      "4  생활/문화\n",
      "5  IT/과학\n",
      "6  세계\n",
      "7  랭킹\n",
      "##########################################\n",
      "원하는 숫자를 입력해주세요.4\n",
      "##########################################\n",
      "1  건강정보 \n",
      "2  자동차/시승기 \n",
      "3  도로/교통 \n",
      "4  여행/레저 \n",
      "5  음식/맛집 \n",
      "6  패션/뷰티 \n",
      "7  공연/전시 \n",
      "8  책 \n",
      "9  종교 \n",
      "10  날씨 \n",
      "11  생활문화 일반 \n",
      "##########################################\n",
      "원하는 숫자를 입력해주세요.4\n",
      "##########################################\n",
      "요약본 1번 / 전체본 2번 선택해주세요.2\n",
      "https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid1=103&여행/레저 \n",
      "##########################################\n",
      "뉴스 크롤링을 시작할 날짜를 입력해주세요.\n",
      "예: 2020010120220323\n",
      "뉴스 크롤링을 끝낼 날짜를 입력해주세요.\n",
      "예: 2020010120220324\n",
      "##########################################\n",
      "https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid1=103&여행/레저 &date=20220323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20220323: 100%|████████████████████████████████████████████████████████████████████████| 68/68 [00:10<00:00,  6.44it/s]\n"
     ]
    }
   ],
   "source": [
    "df = news_crawling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "80c63f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>날짜</th>\n",
       "      <th>헤드라인</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20220323</td>\n",
       "      <td>양주시립회암사지박물관 문화학교 수강생 공모</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20220323</td>\n",
       "      <td>신천지, 20대 대선 이후 공격적인 포교 기승</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20220323</td>\n",
       "      <td>강원역사문화진흥원 설립 유치 발족식 열려</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20220323</td>\n",
       "      <td>양주시립장욱진미술관 청년작가 20명 드로잉 선봬</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20220323</td>\n",
       "      <td>유엔 사무총장 \"5년 내 모두가 기후 조기 경보 혜택받도록 하자\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340</th>\n",
       "      <td>20220323</td>\n",
       "      <td>[차현진의 돈과 세상] [63] 런던보다 8.5시간 빠른 ‘서울시(時)’</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>20220323</td>\n",
       "      <td>전국 흐리고 일부 지역 비…낮 최고 8∼14도[날씨]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <td>20220323</td>\n",
       "      <td>주님, 사순절 기간입니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>20220323</td>\n",
       "      <td>[오늘의 운세] 2022년 3월 23일 띠별 운세</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344</th>\n",
       "      <td>20220323</td>\n",
       "      <td>[녹유 오늘의 운세] 78년생 유명세 탈 수 있는 기회가 잡혀요</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1345 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            날짜                                      헤드라인\n",
       "0     20220323                   양주시립회암사지박물관 문화학교 수강생 공모\n",
       "1     20220323                 신천지, 20대 대선 이후 공격적인 포교 기승\n",
       "2     20220323                    강원역사문화진흥원 설립 유치 발족식 열려\n",
       "3     20220323                양주시립장욱진미술관 청년작가 20명 드로잉 선봬\n",
       "4     20220323      유엔 사무총장 \"5년 내 모두가 기후 조기 경보 혜택받도록 하자\"\n",
       "...        ...                                       ...\n",
       "1340  20220323  [차현진의 돈과 세상] [63] 런던보다 8.5시간 빠른 ‘서울시(時)’\n",
       "1341  20220323             전국 흐리고 일부 지역 비…낮 최고 8∼14도[날씨]\n",
       "1342  20220323                            주님, 사순절 기간입니다.\n",
       "1343  20220323               [오늘의 운세] 2022년 3월 23일 띠별 운세\n",
       "1344  20220323       [녹유 오늘의 운세] 78년생 유명세 탈 수 있는 기회가 잡혀요\n",
       "\n",
       "[1345 rows x 2 columns]"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c6a4b1",
   "metadata": {},
   "source": [
    "## 날짜 데이터 다루기 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "d3ff9178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-11 00:00:00\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.strptime('20200111', \"%Y%m%d\")\n",
    "print(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "e4ccdb11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "08f5dfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-11 00:00:00\n"
     ]
    }
   ],
   "source": [
    "end = datetime.datetime.strptime('20200211', \"%Y%m%d\")\n",
    "print(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "2f2d7361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(days=31)"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "49a97c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(end - start).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "6d8c3ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2020-01-11', '2020-01-12', '2020-01-13', '2020-01-14',\n",
       "               '2020-01-15', '2020-01-16', '2020-01-17', '2020-01-18',\n",
       "               '2020-01-19', '2020-01-20', '2020-01-21', '2020-01-22',\n",
       "               '2020-01-23', '2020-01-24', '2020-01-25', '2020-01-26',\n",
       "               '2020-01-27', '2020-01-28', '2020-01-29', '2020-01-30',\n",
       "               '2020-01-31', '2020-02-01', '2020-02-02', '2020-02-03',\n",
       "               '2020-02-04', '2020-02-05', '2020-02-06', '2020-02-07',\n",
       "               '2020-02-08', '2020-02-09', '2020-02-10'],\n",
       "              dtype='datetime64[ns]', freq='D')"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_generated = pd.date_range(start, periods= (end - start).days)\n",
    "date_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ab6845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fc0a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1886e590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830ce5c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c9d425",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
